{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from uio.unified_io_hf import image_processing_uio as ip_uio \n",
    "import torch\n",
    "from PIL import Image\n",
    "import urllib \n",
    "import numpy as np\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        img = Image.open(f)\n",
    "        return np.array(img)\n",
    "\n",
    "from flax.serialization import from_bytes\n",
    "\n",
    "# add to sys path\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "from torch import nn\n",
    "\n",
    "from uio import network as nw_jax \n",
    "from uio.unified_io_hf import modeling_uio_vae as nw_torch \n",
    "import jax.numpy as jnp \n",
    "import jax\n",
    "\n",
    "from uio.t5x_layers import Conv\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "\n",
    "def convert_params_to_ones(params):\n",
    "    params = dict(params)\n",
    "    for k, v in params.items():\n",
    "        if isinstance(v, jnp.ndarray):\n",
    "            params[k] = jnp.ones_like(v)\n",
    "            print(k, v.shape)\n",
    "        elif isinstance(v, dict) or isinstance(v, FrozenDict): \n",
    "            params[k] = convert_params_to_ones(v)\n",
    "        else:\n",
    "            params[k] = v\n",
    "    return params\n",
    "\n",
    "def load_checkpoint(checkpoint):\n",
    "  \"\"\"Load a bin file as a tree of jax arrays\"\"\"\n",
    "  with open(checkpoint, \"rb\") as state_f:\n",
    "    state = from_bytes(None, state_f.read())\n",
    "  state = jax.tree_util.tree_map(jnp.array, state)\n",
    "  return state\n",
    "\n",
    "\n",
    "def flatten_dict(d):\n",
    "    \"\"\"\n",
    "    Takes an input dict and concatenates all nested keys into a single key using '.' as a separator.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = flatten_dict(v)\n",
    "            for k2, v2 in v.items():\n",
    "                out[k + '.' + k2] = v2\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./ckpts/small.bin\"\n",
    "weights = load_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uio.unified_io_hf.modeling_uio_vae import VAEConfig\n",
    "DTYPE = \"float32\"\n",
    "VAE_CONFIG = VAEConfig(\n",
    "  embed_dim=256,\n",
    "  n_embed=16384,\n",
    "  double_z=False,\n",
    "  z_channels=256,\n",
    "  resolution=256,\n",
    "  in_channels=3,\n",
    "  out_ch=3,\n",
    "  ch=128,\n",
    "  ch_mult=(1,1,2,2,4),\n",
    "  num_res_blocks=2,\n",
    "  attn_resolutions=(16,),\n",
    "  dropout=0,\n",
    "  dtype=DTYPE,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_weights_downsample_block(weights_jax):\n",
    "    weights_torch = {} \n",
    "    for k, v in weights_jax.items():\n",
    "        if \"kernel\" in k:\n",
    "            #gotta be careful and transpose the kernel dims coz their impl is different\n",
    "            weights_torch[k.replace(\"kernel\", \"weight\")] = torch.from_numpy(np.array(v).transpose(3, 2, 0, 1)).float()\n",
    "        elif \"bias\" in k:\n",
    "            weights_torch[k] = torch.from_numpy(np.array(v)).float()\n",
    "    return weights_torch\n",
    "    \n",
    "def translate_weights_upsample_block(weights_jax):\n",
    "    weights_torch = {} \n",
    "    for k, v in weights_jax.items():\n",
    "        if \"kernel\" in k:\n",
    "            #gotta be careful and transpose the kernel dims coz their impl is different\n",
    "            weights_torch[k.replace(\"kernel\", \"weight\")] = torch.from_numpy(np.array(v).transpose(3,2,0,1)).float()\n",
    "        elif \"bias\" in k:\n",
    "            weights_torch[k] = torch.from_numpy(np.array(v)).float()\n",
    "    return weights_torch\n",
    "\n",
    "\n",
    "def translate_weights_res_block(weights_jax):\n",
    "    weights_torch = {}\n",
    "    for k, v in weights_jax.items():\n",
    "        if \"kernel\" in k:\n",
    "            weights_torch[k.replace(\"kernel\", \"weight\")] = torch.from_numpy(np.array(v).transpose(3,2,0,1)).float()\n",
    "        elif \"scale\" in k:\n",
    "            weights_torch[k.replace(\"scale\", \"weight\")] = torch.from_numpy(np.array(v)).float()\n",
    "        elif \"bias\" in k:\n",
    "            weights_torch[k] = torch.from_numpy(np.array(v)).float()\n",
    "    return weights_torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vae encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax (128,) tORCH torch.Size([128]) conv_in.bias\n",
      "Jax (3, 3, 3, 128) tORCH torch.Size([128, 3, 3, 3]) conv_in.weight\n",
      "Jax (256,) tORCH torch.Size([256]) conv_out.bias\n",
      "Jax (3, 3, 512, 256) tORCH torch.Size([256, 512, 3, 3]) conv_out.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.0.conv1.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) res_blocks.0.0.conv1.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.0.conv2.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) res_blocks.0.0.conv2.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.0.norm1.bias\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.0.norm1.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.0.norm2.bias\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.0.norm2.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.1.conv1.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) res_blocks.0.1.conv1.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.1.conv2.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) res_blocks.0.1.conv2.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.1.norm1.bias\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.1.norm1.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.1.norm2.bias\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.0.1.norm2.weight\n",
      "Jax (128,) tORCH torch.Size([128]) downsamples.0.conv.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) downsamples.0.conv.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.0.conv1.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) res_blocks.1.0.conv1.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.0.conv2.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) res_blocks.1.0.conv2.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.0.norm1.bias\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.0.norm1.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.0.norm2.bias\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.0.norm2.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.1.conv1.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) res_blocks.1.1.conv1.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.1.conv2.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) res_blocks.1.1.conv2.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.1.norm1.bias\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.1.norm1.weight\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.1.norm2.bias\n",
      "Jax (128,) tORCH torch.Size([128]) res_blocks.1.1.norm2.weight\n",
      "Jax (128,) tORCH torch.Size([128]) downsamples.1.conv.bias\n",
      "Jax (3, 3, 128, 128) tORCH torch.Size([128, 128, 3, 3]) downsamples.1.conv.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.0.conv1.bias\n",
      "Jax (3, 3, 128, 256) tORCH torch.Size([256, 128, 3, 3]) res_blocks.2.0.conv1.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.0.conv2.bias\n",
      "Jax (3, 3, 256, 256) tORCH torch.Size([256, 256, 3, 3]) res_blocks.2.0.conv2.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.0.nin_shortcut.bias\n",
      "Jax (1, 1, 128, 256) tORCH torch.Size([256, 128, 1, 1]) res_blocks.2.0.nin_shortcut.weight\n",
      "Jax (128,) tORCH torch.Size([256]) res_blocks.2.0.norm1.bias\n",
      "Jax (128,) tORCH torch.Size([256]) res_blocks.2.0.norm1.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.0.norm2.bias\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.0.norm2.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.1.conv1.bias\n",
      "Jax (3, 3, 256, 256) tORCH torch.Size([256, 256, 3, 3]) res_blocks.2.1.conv1.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.1.conv2.bias\n",
      "Jax (3, 3, 256, 256) tORCH torch.Size([256, 256, 3, 3]) res_blocks.2.1.conv2.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.1.norm1.bias\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.1.norm1.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.1.norm2.bias\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.2.1.norm2.weight\n",
      "Jax (256,) tORCH torch.Size([256]) downsamples.2.conv.bias\n",
      "Jax (3, 3, 256, 256) tORCH torch.Size([256, 256, 3, 3]) downsamples.2.conv.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.0.conv1.bias\n",
      "Jax (3, 3, 256, 256) tORCH torch.Size([256, 256, 3, 3]) res_blocks.3.0.conv1.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.0.conv2.bias\n",
      "Jax (3, 3, 256, 256) tORCH torch.Size([256, 256, 3, 3]) res_blocks.3.0.conv2.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.0.norm1.bias\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.0.norm1.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.0.norm2.bias\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.0.norm2.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.1.conv1.bias\n",
      "Jax (3, 3, 256, 256) tORCH torch.Size([256, 256, 3, 3]) res_blocks.3.1.conv1.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.1.conv2.bias\n",
      "Jax (3, 3, 256, 256) tORCH torch.Size([256, 256, 3, 3]) res_blocks.3.1.conv2.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.1.norm1.bias\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.1.norm1.weight\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.1.norm2.bias\n",
      "Jax (256,) tORCH torch.Size([256]) res_blocks.3.1.norm2.weight\n",
      "Jax (256,) tORCH torch.Size([256]) downsamples.3.conv.bias\n",
      "Jax (3, 3, 256, 256) tORCH torch.Size([256, 256, 3, 3]) downsamples.3.conv.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.0.k.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) attn_blocks.4.0.k.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.0.norm.bias\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.0.norm.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.0.proj_out.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) attn_blocks.4.0.proj_out.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.0.q.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) attn_blocks.4.0.q.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.0.v.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) attn_blocks.4.0.v.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.1.k.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) attn_blocks.4.1.k.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.1.norm.bias\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.1.norm.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.1.proj_out.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) attn_blocks.4.1.proj_out.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.1.q.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) attn_blocks.4.1.q.weight\n",
      "Jax (512,) tORCH torch.Size([512]) attn_blocks.4.1.v.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) attn_blocks.4.1.v.weight\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.0.conv1.bias\n",
      "Jax (3, 3, 256, 512) tORCH torch.Size([512, 256, 3, 3]) res_blocks.4.0.conv1.weight\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.0.conv2.bias\n",
      "Jax (3, 3, 512, 512) tORCH torch.Size([512, 512, 3, 3]) res_blocks.4.0.conv2.weight\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.0.nin_shortcut.bias\n",
      "Jax (1, 1, 256, 512) tORCH torch.Size([512, 256, 1, 1]) res_blocks.4.0.nin_shortcut.weight\n",
      "Jax (256,) tORCH torch.Size([512]) res_blocks.4.0.norm1.bias\n",
      "Jax (256,) tORCH torch.Size([512]) res_blocks.4.0.norm1.weight\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.0.norm2.bias\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.0.norm2.weight\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.1.conv1.bias\n",
      "Jax (3, 3, 512, 512) tORCH torch.Size([512, 512, 3, 3]) res_blocks.4.1.conv1.weight\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.1.conv2.bias\n",
      "Jax (3, 3, 512, 512) tORCH torch.Size([512, 512, 3, 3]) res_blocks.4.1.conv2.weight\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.1.norm1.bias\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.1.norm1.weight\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.1.norm2.bias\n",
      "Jax (512,) tORCH torch.Size([512]) res_blocks.4.1.norm2.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_attn_1.k.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) mid_attn_1.k.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_attn_1.norm.bias\n",
      "Jax (512,) tORCH torch.Size([512]) mid_attn_1.norm.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_attn_1.proj_out.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) mid_attn_1.proj_out.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_attn_1.q.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) mid_attn_1.q.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_attn_1.v.bias\n",
      "Jax (1, 1, 512, 512) tORCH torch.Size([512, 512, 1, 1]) mid_attn_1.v.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_1.conv1.bias\n",
      "Jax (3, 3, 512, 512) tORCH torch.Size([512, 512, 3, 3]) mid_block_1.conv1.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_1.conv2.bias\n",
      "Jax (3, 3, 512, 512) tORCH torch.Size([512, 512, 3, 3]) mid_block_1.conv2.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_1.norm1.bias\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_1.norm1.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_1.norm2.bias\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_1.norm2.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_2.conv1.bias\n",
      "Jax (3, 3, 512, 512) tORCH torch.Size([512, 512, 3, 3]) mid_block_2.conv1.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_2.conv2.bias\n",
      "Jax (3, 3, 512, 512) tORCH torch.Size([512, 512, 3, 3]) mid_block_2.conv2.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_2.norm1.bias\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_2.norm1.weight\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_2.norm2.bias\n",
      "Jax (512,) tORCH torch.Size([512]) mid_block_2.norm2.weight\n",
      "Jax (512,) tORCH torch.Size([512]) norm_out.bias\n",
      "Jax (512,) tORCH torch.Size([512]) norm_out.weight\n"
     ]
    }
   ],
   "source": [
    "# given the outputs below, write a function to translate weights from jax to torch\n",
    "\n",
    "def translate_weights_vae_encoder(weights_jax):\n",
    "    weights_torch = {}\n",
    "    for k, v in weights_jax.items():\n",
    "\n",
    "        if k == \"down_2_block_0.norm1.scale\":\n",
    "\n",
    "            flag = True\n",
    "        if \"down_\" in k and (\"block_\" in k or \"attn_\" in k):\n",
    "            idxs = k.split(\".\")[0].split(\"_\")\n",
    "            idxs = [idxs[1], idxs[3]]\n",
    "            if \"down_\" and \"block_\" in k:\n",
    "                k = k.replace(\"down_\" + idxs[0] + \"_block_\" + idxs[1], \"res_blocks.\" + idxs[0] + \".\" + idxs[1])\n",
    "            if \"down_\" and \"attn_\" in k:\n",
    "                k = k.replace(\"down_\" + idxs[0] + \"_attn_\" + idxs[1], \"attn_blocks.\" + idxs[0] + \".\" + idxs[1])\n",
    "\n",
    "\n",
    "        if \"downsample\" in k:\n",
    "            idxs = k.split(\".\")[0].split(\"_\")\n",
    "            idxs = [idxs[1]]\n",
    "            k = k.replace(f\"down_{idxs[0]}_downsample\", f\"downsamples.{idxs[0]}\")\n",
    "        if \"kernel\" in k:\n",
    "            k = k.replace(\"kernel\", \"weight\")\n",
    "            weights_torch[k] = torch.from_numpy(np.array(v).transpose(3,2, 0, 1)).float()\n",
    "        elif \"scale\" in k:\n",
    "            k = k.replace(\"scale\", \"weight\")\n",
    "            weights_torch[k.replace(\"scale\", \"weight\")] = torch.from_numpy(np.array(v)).float()\n",
    "        elif \"bias\" in k:\n",
    "            weights_torch[k] = torch.from_numpy(np.array(v)).float()\n",
    "        print(\"Jax\", v.shape, \"tORCH\", state_dict[k].shape, k)\n",
    "    return weights_torch\n",
    "flat_weights = flatten_dict(weights[\"discrete_vae\"][\"encoder\"])\n",
    "weights_torch= translate_weights_vae_encoder(flat_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_blocks.0.0.norm1.weight torch.Size([128])\n",
      "res_blocks.0.0.norm1.bias torch.Size([128])\n",
      "res_blocks.0.1.norm1.weight torch.Size([128])\n",
      "res_blocks.0.1.norm1.bias torch.Size([128])\n",
      "res_blocks.1.0.norm1.weight torch.Size([128])\n",
      "res_blocks.1.0.norm1.bias torch.Size([128])\n",
      "res_blocks.1.1.norm1.weight torch.Size([128])\n",
      "res_blocks.1.1.norm1.bias torch.Size([128])\n",
      "res_blocks.2.0.norm1.weight torch.Size([256])\n",
      "res_blocks.2.0.norm1.bias torch.Size([256])\n",
      "res_blocks.2.1.norm1.weight torch.Size([256])\n",
      "res_blocks.2.1.norm1.bias torch.Size([256])\n",
      "res_blocks.3.0.norm1.weight torch.Size([256])\n",
      "res_blocks.3.0.norm1.bias torch.Size([256])\n",
      "res_blocks.3.1.norm1.weight torch.Size([256])\n",
      "res_blocks.3.1.norm1.bias torch.Size([256])\n",
      "res_blocks.4.0.norm1.weight torch.Size([512])\n",
      "res_blocks.4.0.norm1.bias torch.Size([512])\n",
      "res_blocks.4.1.norm1.weight torch.Size([512])\n",
      "res_blocks.4.1.norm1.bias torch.Size([512])\n",
      "mid_block_1.norm1.weight torch.Size([512])\n",
      "mid_block_1.norm1.bias torch.Size([512])\n",
      "mid_block_2.norm1.weight torch.Size([512])\n",
      "mid_block_2.norm1.bias torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7168"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = vae_encoder_torch.state_dict()\n",
    "total_sum = 0\n",
    "for k, v in state_dict.items():\n",
    "    if \"norm1\" in k:\n",
    "        print(k, v.shape)\n",
    "        total_sum+=v.shape[0]\n",
    "total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_0_block_0.norm1.bias (128,)\n",
      "down_0_block_0.norm1.scale (128,)\n",
      "down_0_block_1.norm1.bias (128,)\n",
      "down_0_block_1.norm1.scale (128,)\n",
      "down_1_block_0.norm1.bias (128,)\n",
      "down_1_block_0.norm1.scale (128,)\n",
      "down_1_block_1.norm1.bias (128,)\n",
      "down_1_block_1.norm1.scale (128,)\n",
      "down_2_block_0.norm1.bias (128,)\n",
      "down_2_block_0.norm1.scale (128,)\n",
      "down_2_block_1.norm1.bias (256,)\n",
      "down_2_block_1.norm1.scale (256,)\n",
      "down_3_block_0.norm1.bias (256,)\n",
      "down_3_block_0.norm1.scale (256,)\n",
      "down_3_block_1.norm1.bias (256,)\n",
      "down_3_block_1.norm1.scale (256,)\n",
      "down_4_block_0.norm1.bias (256,)\n",
      "down_4_block_0.norm1.scale (256,)\n",
      "down_4_block_1.norm1.bias (512,)\n",
      "down_4_block_1.norm1.scale (512,)\n",
      "mid_block_1.norm1.bias (512,)\n",
      "mid_block_1.norm1.scale (512,)\n",
      "mid_block_2.norm1.bias (512,)\n",
      "mid_block_2.norm1.scale (512,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6400"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_weights = flatten_dict(weights[\"discrete_vae\"][\"encoder\"])\n",
    "total_sum = 0\n",
    "for k, v in flat_weights.items():\n",
    "    if \"norm1\" in k:\n",
    "        print(k, v.shape)\n",
    "        total_sum+=v.shape[0]\n",
    "total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 128\n",
      "128 128\n",
      "128 256\n",
      "256 256\n",
      "256 512\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE_Encoder:\n\tsize mismatch for res_blocks.2.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for res_blocks.2.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for res_blocks.4.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for res_blocks.4.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[741], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vae_encoder_torch \u001b[39m=\u001b[39m nw_torch\u001b[39m.\u001b[39mVAE_Encoder(VAE_CONFIG)\n\u001b[0;32m----> 2\u001b[0m vae_encoder_torch\u001b[39m.\u001b[39;49mload_state_dict(weights_torch)\n",
      "File \u001b[0;32m~/miniconda3/envs/uio/lib/python3.8/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE_Encoder:\n\tsize mismatch for res_blocks.2.0.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for res_blocks.2.0.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for res_blocks.4.0.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for res_blocks.4.0.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512])."
     ]
    }
   ],
   "source": [
    "vae_encoder_torch = nw_torch.VAE_Encoder(VAE_CONFIG)\n",
    "vae_encoder_torch.load_state_dict(weights_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_in.weight torch.Size([128, 3, 3, 3])\n",
      "conv_in.bias torch.Size([128])\n",
      "res_blocks.0.0.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "res_blocks.0.0.conv1.bias torch.Size([128])\n",
      "res_blocks.0.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "res_blocks.0.0.conv2.bias torch.Size([128])\n",
      "res_blocks.0.0.norm1.weight torch.Size([128])\n",
      "res_blocks.0.0.norm1.bias torch.Size([128])\n",
      "res_blocks.0.0.norm2.weight torch.Size([128])\n",
      "res_blocks.0.0.norm2.bias torch.Size([128])\n",
      "res_blocks.0.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "res_blocks.0.1.conv1.bias torch.Size([128])\n",
      "res_blocks.0.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "res_blocks.0.1.conv2.bias torch.Size([128])\n",
      "res_blocks.0.1.norm1.weight torch.Size([128])\n",
      "res_blocks.0.1.norm1.bias torch.Size([128])\n",
      "res_blocks.0.1.norm2.weight torch.Size([128])\n",
      "res_blocks.0.1.norm2.bias torch.Size([128])\n",
      "res_blocks.1.0.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "res_blocks.1.0.conv1.bias torch.Size([128])\n",
      "res_blocks.1.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "res_blocks.1.0.conv2.bias torch.Size([128])\n",
      "res_blocks.1.0.norm1.weight torch.Size([128])\n",
      "res_blocks.1.0.norm1.bias torch.Size([128])\n",
      "res_blocks.1.0.norm2.weight torch.Size([128])\n",
      "res_blocks.1.0.norm2.bias torch.Size([128])\n",
      "res_blocks.1.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "res_blocks.1.1.conv1.bias torch.Size([128])\n",
      "res_blocks.1.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "res_blocks.1.1.conv2.bias torch.Size([128])\n",
      "res_blocks.1.1.norm1.weight torch.Size([128])\n",
      "res_blocks.1.1.norm1.bias torch.Size([128])\n",
      "res_blocks.1.1.norm2.weight torch.Size([128])\n",
      "res_blocks.1.1.norm2.bias torch.Size([128])\n",
      "res_blocks.2.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "res_blocks.2.0.conv1.bias torch.Size([256])\n",
      "res_blocks.2.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "res_blocks.2.0.conv2.bias torch.Size([256])\n",
      "res_blocks.2.0.norm1.weight torch.Size([256])\n",
      "res_blocks.2.0.norm1.bias torch.Size([256])\n",
      "res_blocks.2.0.norm2.weight torch.Size([256])\n",
      "res_blocks.2.0.norm2.bias torch.Size([256])\n",
      "res_blocks.2.0.nin_shortcut.weight torch.Size([256, 128, 1, 1])\n",
      "res_blocks.2.0.nin_shortcut.bias torch.Size([256])\n",
      "res_blocks.2.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "res_blocks.2.1.conv1.bias torch.Size([256])\n",
      "res_blocks.2.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "res_blocks.2.1.conv2.bias torch.Size([256])\n",
      "res_blocks.2.1.norm1.weight torch.Size([256])\n",
      "res_blocks.2.1.norm1.bias torch.Size([256])\n",
      "res_blocks.2.1.norm2.weight torch.Size([256])\n",
      "res_blocks.2.1.norm2.bias torch.Size([256])\n",
      "res_blocks.3.0.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "res_blocks.3.0.conv1.bias torch.Size([256])\n",
      "res_blocks.3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "res_blocks.3.0.conv2.bias torch.Size([256])\n",
      "res_blocks.3.0.norm1.weight torch.Size([256])\n",
      "res_blocks.3.0.norm1.bias torch.Size([256])\n",
      "res_blocks.3.0.norm2.weight torch.Size([256])\n",
      "res_blocks.3.0.norm2.bias torch.Size([256])\n",
      "res_blocks.3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "res_blocks.3.1.conv1.bias torch.Size([256])\n",
      "res_blocks.3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "res_blocks.3.1.conv2.bias torch.Size([256])\n",
      "res_blocks.3.1.norm1.weight torch.Size([256])\n",
      "res_blocks.3.1.norm1.bias torch.Size([256])\n",
      "res_blocks.3.1.norm2.weight torch.Size([256])\n",
      "res_blocks.3.1.norm2.bias torch.Size([256])\n",
      "res_blocks.4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "res_blocks.4.0.conv1.bias torch.Size([512])\n",
      "res_blocks.4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "res_blocks.4.0.conv2.bias torch.Size([512])\n",
      "res_blocks.4.0.norm1.weight torch.Size([512])\n",
      "res_blocks.4.0.norm1.bias torch.Size([512])\n",
      "res_blocks.4.0.norm2.weight torch.Size([512])\n",
      "res_blocks.4.0.norm2.bias torch.Size([512])\n",
      "res_blocks.4.0.nin_shortcut.weight torch.Size([512, 256, 1, 1])\n",
      "res_blocks.4.0.nin_shortcut.bias torch.Size([512])\n",
      "res_blocks.4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "res_blocks.4.1.conv1.bias torch.Size([512])\n",
      "res_blocks.4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "res_blocks.4.1.conv2.bias torch.Size([512])\n",
      "res_blocks.4.1.norm1.weight torch.Size([512])\n",
      "res_blocks.4.1.norm1.bias torch.Size([512])\n",
      "res_blocks.4.1.norm2.weight torch.Size([512])\n",
      "res_blocks.4.1.norm2.bias torch.Size([512])\n",
      "attn_blocks.4.0.norm.weight torch.Size([512])\n",
      "attn_blocks.4.0.norm.bias torch.Size([512])\n",
      "attn_blocks.4.0.q.weight torch.Size([512, 512, 1, 1])\n",
      "attn_blocks.4.0.q.bias torch.Size([512])\n",
      "attn_blocks.4.0.k.weight torch.Size([512, 512, 1, 1])\n",
      "attn_blocks.4.0.k.bias torch.Size([512])\n",
      "attn_blocks.4.0.v.weight torch.Size([512, 512, 1, 1])\n",
      "attn_blocks.4.0.v.bias torch.Size([512])\n",
      "attn_blocks.4.0.proj_out.weight torch.Size([512, 512, 1, 1])\n",
      "attn_blocks.4.0.proj_out.bias torch.Size([512])\n",
      "attn_blocks.4.1.norm.weight torch.Size([512])\n",
      "attn_blocks.4.1.norm.bias torch.Size([512])\n",
      "attn_blocks.4.1.q.weight torch.Size([512, 512, 1, 1])\n",
      "attn_blocks.4.1.q.bias torch.Size([512])\n",
      "attn_blocks.4.1.k.weight torch.Size([512, 512, 1, 1])\n",
      "attn_blocks.4.1.k.bias torch.Size([512])\n",
      "attn_blocks.4.1.v.weight torch.Size([512, 512, 1, 1])\n",
      "attn_blocks.4.1.v.bias torch.Size([512])\n",
      "attn_blocks.4.1.proj_out.weight torch.Size([512, 512, 1, 1])\n",
      "attn_blocks.4.1.proj_out.bias torch.Size([512])\n",
      "downsamples.0.conv.weight torch.Size([128, 128, 3, 3])\n",
      "downsamples.0.conv.bias torch.Size([128])\n",
      "downsamples.1.conv.weight torch.Size([128, 128, 3, 3])\n",
      "downsamples.1.conv.bias torch.Size([128])\n",
      "downsamples.2.conv.weight torch.Size([256, 256, 3, 3])\n",
      "downsamples.2.conv.bias torch.Size([256])\n",
      "downsamples.3.conv.weight torch.Size([256, 256, 3, 3])\n",
      "downsamples.3.conv.bias torch.Size([256])\n",
      "mid_block_1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "mid_block_1.conv1.bias torch.Size([512])\n",
      "mid_block_1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "mid_block_1.conv2.bias torch.Size([512])\n",
      "mid_block_1.norm1.weight torch.Size([512])\n",
      "mid_block_1.norm1.bias torch.Size([512])\n",
      "mid_block_1.norm2.weight torch.Size([512])\n",
      "mid_block_1.norm2.bias torch.Size([512])\n",
      "mid_attn_1.norm.weight torch.Size([512])\n",
      "mid_attn_1.norm.bias torch.Size([512])\n",
      "mid_attn_1.q.weight torch.Size([512, 512, 1, 1])\n",
      "mid_attn_1.q.bias torch.Size([512])\n",
      "mid_attn_1.k.weight torch.Size([512, 512, 1, 1])\n",
      "mid_attn_1.k.bias torch.Size([512])\n",
      "mid_attn_1.v.weight torch.Size([512, 512, 1, 1])\n",
      "mid_attn_1.v.bias torch.Size([512])\n",
      "mid_attn_1.proj_out.weight torch.Size([512, 512, 1, 1])\n",
      "mid_attn_1.proj_out.bias torch.Size([512])\n",
      "mid_block_2.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "mid_block_2.conv1.bias torch.Size([512])\n",
      "mid_block_2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "mid_block_2.conv2.bias torch.Size([512])\n",
      "mid_block_2.norm1.weight torch.Size([512])\n",
      "mid_block_2.norm1.bias torch.Size([512])\n",
      "mid_block_2.norm2.weight torch.Size([512])\n",
      "mid_block_2.norm2.bias torch.Size([512])\n",
      "norm_out.weight torch.Size([512])\n",
      "norm_out.bias torch.Size([512])\n",
      "conv_out.weight torch.Size([256, 512, 3, 3])\n",
      "conv_out.bias torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "state_dict = vae_encoder_torch.state_dict()\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downsample block done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_weights = flatten_dict(weights[\"discrete_vae\"][\"encoder\"][\"down_1_downsample\"])\n",
    "weights_torch = translate_weights_downsample_block(flat_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "image_input = np.random.randn(1, 16, 16, 128)\n",
    "image_input_torch = torch.from_numpy(image_input).float()\n",
    "image_input_torch = image_input_torch.permute(0, 3, 1, 2)\n",
    "downsample = nw_torch.Downsample(128)\n",
    "downsample.load_state_dict(weights_torch, strict=False)\n",
    "output_torch = downsample(image_input_torch).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_jax = nw_jax.Downsample(128)\n",
    "key = jax.random.PRNGKey(0)\n",
    "# init the module with constant weights\n",
    "params = downsample_jax.init(key, image_input)\n",
    "params = dict(params)\n",
    "params[\"params\"] = weights[\"discrete_vae\"][\"encoder\"][\"down_1_downsample\"]\n",
    "output_jax = downsample_jax.apply(params, image_input, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 128, 8, 8), (1, 8, 8, 128))"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch.shape, output_jax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00028494012"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch = output_torch.transpose(0, 2, 3, 1)\n",
    "\n",
    "np.abs(output_torch - output_jax).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# res block, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "flat_weights = flatten_dict(weights[\"discrete_vae\"][\"encoder\"][\"mid_block_1\"])\n",
    "weights_torch = translate_weights_res_block(flat_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias (512,)\n",
      "conv1.kernel (3, 3, 512, 512)\n",
      "conv2.bias (512,)\n",
      "conv2.kernel (3, 3, 512, 512)\n",
      "norm1.bias (512,)\n",
      "norm1.scale (512,)\n",
      "norm2.bias (512,)\n",
      "norm2.scale (512,)\n"
     ]
    }
   ],
   "source": [
    "for k, v in flat_weights.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 16, 16)"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_input = np.random.randn(1, 16, 16, 512)\n",
    "image_input_torch = torch.from_numpy(image_input).float()\n",
    "image_input_torch = image_input_torch.permute(0, 3, 1, 2)\n",
    "res_block_torch = nw_torch.ResBlock(512, 512)\n",
    "res_block_torch.load_state_dict(weights_torch, strict=True)\n",
    "output_torch = res_block_torch(image_input_torch).detach().numpy()\n",
    "output_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block_jax = nw_jax.ResBlock(512, 512)\n",
    "key = jax.random.PRNGKey(0)\n",
    "# init the module with constant weights\n",
    "params = res_block_jax.init(key, image_input)\n",
    "params = dict(params)\n",
    "params[\"params\"] = weights[\"discrete_vae\"][\"encoder\"][\"mid_block_1\"]\n",
    "output_jax = res_block_jax.apply(params, image_input, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1574694e-05"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch = output_torch.transpose(0, 2, 3, 1)\n",
    "\n",
    "np.abs(output_torch - output_jax).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample bloc, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_weights = flatten_dict(weights[\"discrete_vae\"][\"decoder\"][\"up_1_upsample\"])\n",
    "weights_torch = translate_weights_upsample_block(flat_weights)\n",
    "np.random.seed(3)\n",
    "image_input = np.random.randn(1, 16, 16, 256)\n",
    "image_input_torch = torch.from_numpy(image_input).float()\n",
    "image_input_torch = image_input_torch.permute(0, 3, 1, 2)\n",
    "upsample = nw_torch.Upsample(256)\n",
    "upsample.load_state_dict(weights_torch, strict=False)\n",
    "output_torch = upsample(image_input_torch).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_jax = nw_jax.Upsample(256)\n",
    "key = jax.random.PRNGKey(0)\n",
    "# init the module with constant weights\n",
    "params = upsample_jax.init(key, image_input)\n",
    "params = dict(params)\n",
    "params[\"params\"] = weights[\"discrete_vae\"][\"decoder\"][\"up_1_upsample\"]\n",
    "output_jax = upsample_jax.apply(params, image_input, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.232075e-07"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch = output_torch.transpose(0, 2, 3, 1)\n",
    "\n",
    "np.abs(output_torch - output_jax).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing attnblock, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_weights_attn_block(weights_jax):\n",
    "    # v is a jax numpy array, turn it into a numpy array\n",
    "    weights_torch = {}\n",
    "    for k, v in weights_jax.items():\n",
    "        if \"kernel\" in k:\n",
    "            weights_torch[k.replace(\"kernel\", \"weight\")] = torch.from_numpy(np.array(v).transpose(3,2, 0, 1)).float()\n",
    "        elif \"scale\" in k:\n",
    "            weights_torch[k.replace(\"scale\", \"weight\")] = torch.from_numpy(np.array(v)).float()\n",
    "        elif \"bias\" in k:\n",
    "            weights_torch[k] = torch.from_numpy(np.array(v)).float()\n",
    "    return weights_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_attn_jax = weights[\"discrete_vae\"][\"encoder\"][\"mid_attn_1\"]\n",
    "weights_attn_torch = translate_weights_attn_block(flatten_dict(weights[\"discrete_vae\"][\"encoder\"][\"mid_attn_1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy random seed \n",
    "np.random.seed(5)\n",
    "image_input = np.random.randn(1, 12, 12, 512).astype(np.float32)*10\n",
    "image_input_torch = torch.from_numpy(image_input).float()\n",
    "\n",
    "attn_block_torch = nw_torch.AttnBlock(512)\n",
    "\n",
    "# set all weights to 1\n",
    "attn_block_torch.load_state_dict(weights_attn_torch)\n",
    "# put in channels first \n",
    "image_input_torch = image_input_torch.permute(0, 3, 1, 2)\n",
    "output_torch = attn_block_torch(image_input_torch).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = attn_block_torch.state_dict()\n",
    "# for key in state_dict.keys():\n",
    "#     print(key, state_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 12, 12, 512), (1, 512, 12, 12))"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test forward on jax, on image_input,\n",
    "\n",
    "attn_block_jax = nw_jax.AttnBlock(512)\n",
    "key = jax.random.PRNGKey(0)\n",
    "# init the module with constant weights\n",
    "params = attn_block_jax.init(key, image_input)\n",
    "params = dict(params)\n",
    "params[\"params\"] = weights_attn_jax\n",
    "output_jax = attn_block_jax.apply(params, image_input, )\n",
    "output_jax.shape, output_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jax = output_jax.transpose(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-39139.152, -39131.5  , -39132.508, -39140.992, -39149.39 ,\n",
       "         -39158.906, -39147.14 ,   9580.306, -39163.58 , -39140.184,\n",
       "         -39138.94 , -39151.848],\n",
       "        [-39127.1  , -39147.492, -39154.066, -39141.39 , -39139.6  ,\n",
       "         -39145.734, -39140.53 ,   9573.505, -39150.555, -39151.242,\n",
       "         -39130.66 , -39138.492],\n",
       "        [-39148.37 , -39109.05 , -39135.023,   9574.731, -39156.473,\n",
       "         -39143.582, -39156.36 , -39155.285, -39155.53 , -39129.875,\n",
       "         -39136.133, -39135.23 ],\n",
       "        [-39145.9  , -39159.402, -39151.387, -39126.402, -39152.523,\n",
       "         -39136.97 , -39153.82 , -39141.363, -39150.344, -39133.797,\n",
       "         -39133.797, -39153.863],\n",
       "        [-39135.156, -39147.402, -39155.117, -39127.855, -39140.11 ,\n",
       "         -39160.223, -39162.7  , -39144.88 , -39153.9  , -39164.562,\n",
       "         -39149.133, -39143.164],\n",
       "        [-39166.375, -39158.64 , -39155.27 , -39130.42 , -39145.188,\n",
       "         -39135.383, -39131.742, -39140.27 , -39158.87 , -39151.254,\n",
       "         -39153.645, -39130.92 ],\n",
       "        [-39138.188, -39153.027, -39131.8  , -39142.67 , -39152.34 ,\n",
       "         -39142.668, -39140.688, -39134.36 , -39145.574, -39140.062,\n",
       "         -39156.46 , -39168.28 ],\n",
       "        [-39135.133, -39146.617, -39148.848, -39143.855, -39137.656,\n",
       "         -39125.746, -39151.613, -39141.723, -39139.86 ,   9576.144,\n",
       "         -39140.52 , -39130.03 ],\n",
       "        [-39138.766, -39129.164, -39132.63 , -39150.496, -39144.105,\n",
       "         -39150.9  , -39151.652, -39143.527, -39130.44 , -39137.008,\n",
       "         -39154.28 , -39148.887],\n",
       "        [-39145.465, -39116.797, -39143.055, -39133.734, -39164.523,\n",
       "         -39129.31 , -39131.637, -39145.844,   9576.635, -39144.117,\n",
       "         -39134.37 , -39154.656],\n",
       "        [-39148.05 , -39132.   , -39145.547, -39143.797, -39147.13 ,\n",
       "         -39152.49 , -39142.273, -39154.992, -39136.2  , -39146.92 ,\n",
       "         -39143.926, -39141.797],\n",
       "        [-39147.203, -39157.23 , -39152.496, -39132.87 , -39136.64 ,\n",
       "         -39143.098, -39137.805, -39140.465, -39139.24 , -39141.535,\n",
       "         -39148.434, -39151.863]], dtype=float32),\n",
       " Array([[-39138.79 , -39131.137, -39132.145, -39140.63 , -39149.027,\n",
       "         -39158.543, -39146.777,   9580.324, -39163.215, -39139.82 ,\n",
       "         -39138.58 , -39151.484],\n",
       "        [-39126.74 , -39147.13 , -39153.703, -39141.027, -39139.24 ,\n",
       "         -39145.37 , -39140.168,   9573.523, -39150.19 , -39150.88 ,\n",
       "         -39130.297, -39139.508],\n",
       "        [-39148.008, -39108.688, -39134.66 ,   9574.75 , -39156.11 ,\n",
       "         -39143.22 , -39155.996, -39154.92 , -39155.168, -39129.51 ,\n",
       "         -39135.77 , -39134.867],\n",
       "        [-39145.535, -39159.04 , -39151.023, -39126.04 , -39152.16 ,\n",
       "         -39136.605, -39153.457, -39141.   , -39149.98 , -39133.434,\n",
       "         -39133.434, -39153.5  ],\n",
       "        [-39134.793, -39147.04 , -39154.754, -39127.492, -39139.746,\n",
       "         -39159.86 , -39162.336, -39144.516, -39153.535, -39164.2  ,\n",
       "         -39148.77 , -39142.8  ],\n",
       "        [-39166.01 , -39158.277, -39154.906, -39130.06 , -39144.824,\n",
       "         -39135.02 , -39131.38 , -39139.906, -39158.508, -39150.89 ,\n",
       "         -39153.28 , -39130.56 ],\n",
       "        [-39137.824, -39152.664, -39131.438, -39142.31 , -39151.977,\n",
       "         -39142.305, -39140.324, -39133.996, -39145.21 , -39139.7  ,\n",
       "         -39156.098, -39167.918],\n",
       "        [-39134.77 , -39146.254, -39148.484, -39143.492, -39137.293,\n",
       "         -39125.383, -39151.25 , -39141.36 , -39139.496,   9576.162,\n",
       "         -39140.156, -39129.668],\n",
       "        [-39138.402, -39128.96 , -39132.266, -39150.133, -39143.742,\n",
       "         -39150.535, -39151.29 , -39143.164, -39130.08 , -39136.645,\n",
       "         -39153.918, -39148.523],\n",
       "        [-39145.1  , -39116.434, -39142.69 , -39133.37 , -39164.16 ,\n",
       "         -39128.945, -39131.273, -39145.48 ,   9576.653, -39143.754,\n",
       "         -39134.008, -39154.293],\n",
       "        [-39147.688, -39131.637, -39145.184, -39143.434, -39146.766,\n",
       "         -39152.125, -39141.91 , -39154.63 , -39135.836, -39146.56 ,\n",
       "         -39143.562, -39141.434],\n",
       "        [-39146.84 , -39156.867, -39152.133, -39132.508, -39136.277,\n",
       "         -39142.734, -39137.44 , -39140.1  , -39138.875, -39141.17 ,\n",
       "         -39148.07 , -39151.5  ]], dtype=float32))"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch[0][0], output_jax[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 12, 12)"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35482204"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(output_torch - output_jax).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_jax = DiscreteVAEjax(VAE_CONFIG)\n",
    "# init the module's weights\n",
    "import jax \n",
    "# import jax np \n",
    "\n",
    "vae_model_jax.init(jax.random.PRNGKey(0), processed[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Array' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m zeros \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m), dtype\u001b[39m=\u001b[39mDTYPE)\n\u001b[0;32m----> 5\u001b[0m vae_model_jax\u001b[39m.\u001b[39;49mapply(zeros);\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/uio/lib/python3.8/site-packages/flax/core/scope.py:965\u001b[0m, in \u001b[0;36m_is_valid_variables\u001b[0;34m(variables)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_valid_variables\u001b[39m(variables: VariableDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    957\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Checks whether the given variable dict is valid.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39m    True if `variables` is a valid variable dict.\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 965\u001b[0m   \u001b[39mfor\u001b[39;00m name, col \u001b[39min\u001b[39;00m variables\u001b[39m.\u001b[39;49mitems():\n\u001b[1;32m    966\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    967\u001b[0m       \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Array' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# test vae model on a single image made of zeros \n",
    "import jax.numpy as jnp\n",
    "\n",
    "zeros = jnp.zeros((1, 3, 256, 256), dtype=DTYPE)\n",
    "vae_model_jax.apply(zeros);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = DiscreteVAE(VAE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_img = torch.randn(1, 3, 256, 256, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "in_channels = 512\n",
    "out_channels = 512 \n",
    "\n",
    "input_img = np.random.randn(1, 512, 4, 4).astype(np.float32)\n",
    "input_img_torch = torch.from_numpy(input_img).float()\n",
    "\n",
    "input_img_jax = input_img.transpose(0, 2, 3, 1)\n",
    "\n",
    "print(input_img_jax.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=\"same\", bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = conv2d.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_jax = Conv(features=512,\n",
    "            kernel_size=(1, 1),\n",
    "            dtype=float,\n",
    "            kernel_axes=(\"axis_0\", \"axis_1\", \"axis_2\", \"axis_3\"),\n",
    "            bias_axes=(\"axis_3\",),\n",
    "            name=\"k\",\n",
    "            )\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "# init the module with constant weights\n",
    "params = conv2d_jax.init(key, input_img_jax)\n",
    "params = dict(params)\n",
    "params[\"params\"] = dict(params[\"params\"])\n",
    "params[\"params\"][\"kernel\"] =  jnp.array(weight.transpose(2, 3, 1, 0))\n",
    "output_jax = conv2d_jax.apply(params, input_img_jax, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_torch = conv2d(input_img_torch).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 512, 4, 4), (1, 4, 4, 512))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch.shape, output_jax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00013946078"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch = output_torch.transpose(0, 2, 3, 1)\n",
    "\n",
    "np.abs(output_torch - output_jax).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.37397608, -0.18934363, -0.19102705, ..., -0.11944845,\n",
       "          -0.20906502, -1.5298786 ],\n",
       "         [ 0.31949556,  1.0684662 , -0.07598345, ..., -0.3701464 ,\n",
       "           0.22494976, -0.8975331 ],\n",
       "         [-0.541537  , -0.8052794 , -0.18761101, ...,  0.5390114 ,\n",
       "          -0.44420135,  0.03182103],\n",
       "         [-0.38844013, -0.20471531,  0.18559667, ...,  0.19249475,\n",
       "          -1.0414706 ,  0.54464597]],\n",
       "\n",
       "        [[ 0.473306  ,  0.93011314, -0.35211036, ..., -0.4534372 ,\n",
       "           0.63449764, -1.146805  ],\n",
       "         [-0.25913465,  0.5615176 ,  0.58334273, ..., -0.46822223,\n",
       "          -0.16222015,  0.27533892],\n",
       "         [ 0.01665792,  0.21350853,  0.43480274, ...,  0.11611384,\n",
       "          -0.42272973,  0.41423053],\n",
       "         [ 0.60471356,  0.19033806,  0.14340717, ...,  0.3603496 ,\n",
       "           0.9142468 , -0.82812697]],\n",
       "\n",
       "        [[ 0.50708675,  0.25339413,  0.1398566 , ..., -0.14297803,\n",
       "           0.71717924,  0.20662865],\n",
       "         [ 0.5424281 , -1.0149813 ,  0.61394894, ...,  0.3543902 ,\n",
       "           0.8918604 ,  1.2778537 ],\n",
       "         [-0.14175493,  0.1861202 ,  0.12961918, ..., -0.43532002,\n",
       "          -0.5655488 , -0.35857648],\n",
       "         [-0.00495851,  0.8594774 , -0.23559326, ...,  0.7521536 ,\n",
       "           0.22222553,  0.04488599]],\n",
       "\n",
       "        [[ 0.5666274 , -0.23180896, -0.60258687, ...,  0.12506428,\n",
       "           0.68719876, -0.07067993],\n",
       "         [ 0.44827455, -0.28964993, -1.3930256 , ...,  0.4842843 ,\n",
       "          -0.14963232,  0.3555627 ],\n",
       "         [-0.64093256,  0.3265506 , -0.37242556, ...,  0.9392718 ,\n",
       "          -0.4501009 , -0.37527746],\n",
       "         [-0.24724866, -0.22235216, -0.21378067, ..., -0.9626758 ,\n",
       "          -1.0195198 ,  0.07589325]]]], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8edc44d19ff76d2058a38b71217d1a7ef70fb1011ec7920cb96ce198d61c0376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
